{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def score_distillation_sampling(image, text_prompt, num_steps=100, lr=0.01):\n",
    "    # Convert the input image to a tensor, ensure it's on the correct device, and requires gradient\n",
    "    image_tensor = transforms.ToTensor()(image).unsqueeze(0)\n",
    "    image_tensor = image_tensor.to(device)\n",
    "    \n",
    "    # Ensure that the tensor requires gradient\n",
    "    image_tensor = Variable(image_tensor, requires_grad=True)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for step in range(num_steps):\n",
    "        # Forward pass through the diffusion model with guiding text\n",
    "        output = pipe(prompt=text_prompt, image=image_tensor)['images'][0]\n",
    "        \n",
    "        # Convert output to a tensor and ensure it's on the same device\n",
    "        output_tensor = transforms.ToTensor()(output).unsqueeze(0)\n",
    "        output_tensor = output_tensor.to(image_tensor.device)\n",
    "        \n",
    "        # Resize the output tensor to match the input tensor dimensions\n",
    "        output_tensor_resized = F.interpolate(output_tensor, size=image_tensor.shape[2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Compute the gradient of the loss with respect to the image\n",
    "        loss = ((output_tensor_resized - image_tensor) ** 2).mean()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update image tensor with the gradient\n",
    "        with torch.no_grad():\n",
    "            image_tensor -= lr * image_tensor.grad\n",
    "        \n",
    "        # Zero the gradients for the next iteration\n",
    "        image_tensor.grad.zero_()\n",
    "\n",
    "    # Convert the final tensor back to an image for display or further processing\n",
    "    transformed_image = transforms.ToPILImage()(image_tensor.squeeze().cpu())\n",
    "    return transformed_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  5.94it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m     19\u001b[0m \u001b[39m# Enable model CPU offloading if on GPU\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m pipe\u001b[39m.\u001b[39;49menable_model_cpu_offload()\n\u001b[1;32m     21\u001b[0m \u001b[39m# pipe = pipe.to(device)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdisplay_image\u001b[39m(image):\n\u001b[1;32m     24\u001b[0m     \u001b[39m# If the image is already a PIL image, just display it directly\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/styleganfusion/lib/python3.10/site-packages/diffusers/pipelines/pipeline_utils.py:1082\u001b[0m, in \u001b[0;36mDiffusionPipeline.enable_model_cpu_offload\u001b[0;34m(self, gpu_id, device)\u001b[0m\n\u001b[1;32m   1079\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1081\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclude_from_cpu_offload:\n\u001b[0;32m-> 1082\u001b[0m     model\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m   1083\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1084\u001b[0m     _, hook \u001b[39m=\u001b[39m cpu_offload_with_hook(model, device)\n",
      "File \u001b[0;32m~/anaconda3/envs/styleganfusion/lib/python3.10/site-packages/transformers/modeling_utils.py:2883\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2878\u001b[0m     \u001b[39mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2879\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2880\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2881\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2882\u001b[0m         )\n\u001b[0;32m-> 2883\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mto(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/styleganfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[39mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[0;32m~/anaconda3/envs/styleganfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/styleganfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (4 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/styleganfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[39mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/styleganfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[39m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/styleganfusion/lib/python3.10/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[39m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m   1160\u001b[0m         device,\n\u001b[1;32m   1161\u001b[0m         dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1162\u001b[0m         non_blocking,\n\u001b[1;32m   1163\u001b[0m     )\n\u001b[1;32m   1164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mstr\u001b[39m(e) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot copy out of meta tensor; no data!\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# Adaptation Process for Score Distillation Sampling (SDS) with Stable Diffusion V1.4\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "model_id = \"CompVis/stable-diffusion-v1-4\"\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "torch.cuda.empty_cache()\n",
    "# Enable model CPU offloading if on GPU\n",
    "pipe.enable_model_cpu_offload()\n",
    "# pipe = pipe.to(device)\n",
    "\n",
    "def display_image(image):\n",
    "    # If the image is already a PIL image, just display it directly\n",
    "    if isinstance(image, torch.Tensor):\n",
    "        image_np = image.squeeze().permute(1, 2, 0).cpu().detach().numpy()\n",
    "        plt.imshow(np.clip(image_np, 0, 1))\n",
    "    else:\n",
    "        plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Load a sample image (Here, a random image is generated for demonstration)\n",
    "def generate_random_image(height, width):\n",
    "    return np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)\n",
    "\n",
    "# Run the basic SDS task\n",
    "initial_image = generate_random_image(512, 512)\n",
    "text_prompt = \"A beautiful landscape painting of mountains during sunset\"\n",
    "transformed_image = score_distillation_sampling(initial_image, text_prompt)\n",
    "\n",
    "# Display the final transformed image\n",
    "display_image(transformed_image)\n",
    "\n",
    "# # Bonus Test 1: Experiment with different guiding signals (e.g., different queries)\n",
    "# alternative_prompt = \"A futuristic cityscape at night\"\n",
    "# alternative_transformed_image = score_distillation_sampling(initial_image, alternative_prompt)\n",
    "# display_image(alternative_transformed_image)\n",
    "\n",
    "# # Bonus Test 2: Experiment with corrupted images (e.g., zero out some pixels)\n",
    "# def corrupt_image(image, corruption_type=\"zero_out\"):\n",
    "#     corrupted_image = image.copy()\n",
    "#     if corruption_type == \"zero_out\":\n",
    "#         corrupted_image[::2, ::2] = 0\n",
    "#     elif corruption_type == \"color_jitter\":\n",
    "#         corrupted_image = (corrupted_image + np.random.randint(-30, 30, image.shape)) % 256\n",
    "#     return corrupted_image\n",
    "\n",
    "corrupted_image = corrupt_image(initial_image, \"zero_out\")\n",
    "corrupted_transformed_image = score_distillation_sampling(corrupted_image, text_prompt)\n",
    "display_image(corrupted_transformed_image)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "styleganfusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
